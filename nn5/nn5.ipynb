{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac49a10d",
   "metadata": {},
   "source": [
    "# NN4\n",
    "Tymoteusz Urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530cdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8e79e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    \"\"\"\n",
    "    Implementation of multi layer perceptron\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    model_type : str\n",
    "        model type, regressor or classifier\n",
    "    layers : List\n",
    "        list of layer sizes\n",
    "    num_layers : int\n",
    "        number of layers\n",
    "    init_function : func\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, initalization='xavier', model_type='regression', weights=None, biases=None, activations=None):\n",
    "        \"\"\"\n",
    "        activations - list of available functions: 'sigmoid', 'linear', 'tanh', 'relu', 'softmax' ('softmax' can be used only on the last layer)\n",
    "        initialization - available types: 'xavier', 'he', 'uniform'\n",
    "        model_type - available types: 'regression', 'classification'\n",
    "        \"\"\"        \n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        \n",
    "        initialization_functions = {\n",
    "            'xavier': self.xavier_init,\n",
    "            'he': self.he_init,\n",
    "            'uniform': self.uniform_init\n",
    "        }\n",
    "        self.init_function = initialization_functions.get(initalization)\n",
    "        \n",
    "        assert model_type in ['regression', 'classification']\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = [self.init_function(layers[i-1], layers[i]) for i in range(1, self.num_layers)]\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if biases is None:\n",
    "            self.biases = [self.init_function(layers[i]) for i in range(1, self.num_layers)]\n",
    "        else:\n",
    "            self.biases = biases\n",
    "        \n",
    "        if activations is None:\n",
    "            if self.model_type == 'regression':\n",
    "                self.activations = ['sigmoid' for i in range(1, self.num_layers - 1)] + ['linear']\n",
    "            elif self.model_type == 'classification':\n",
    "                self.activations = ['sigmoid' for i in range(1, self.num_layers - 1)] + ['softmax']\n",
    "        else:\n",
    "            self.activations = activations\n",
    "        \n",
    "        self.velocities_weights = [self.zeros_init(layers[i-1], layers[i]) for i in range(1, self.num_layers)]\n",
    "        self.velocities_biases = [self.zeros_init(layers[i]) for i in range(1, self.num_layers)]\n",
    "        \n",
    "        activation_functions = {\n",
    "            'sigmoid': self._sigmoid,\n",
    "            'linear': self._linear,\n",
    "            'softmax': self._softmax,\n",
    "            'tanh': self._tanh,\n",
    "            'relu': self._relu\n",
    "        }\n",
    "        self.activation_funcs = list(map(lambda x: activation_functions.get(x), self.activations))\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _linear(self, z):\n",
    "        return z\n",
    "    \n",
    "    def _tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    def _relu(self, z):\n",
    "        return np.maximum(z, 0)\n",
    "    \n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def tanh_derivative(self, a):\n",
    "        \"\"\"calculates tanh'(z) where a = tanh(z)\"\"\"\n",
    "        return 1 - a ** 2\n",
    "    \n",
    "    def sigmoid_derivative(self, a):\n",
    "        \"\"\"calculates sigm'(z) where a = sigm(z)\"\"\"\n",
    "        return a * (1-a)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return np.maximum(0, np.sign(z))\n",
    "    \n",
    "    def xavier_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            n_out = n_in\n",
    "            variance = 1 / n_out\n",
    "            stddev = np.sqrt(variance)\n",
    "            return np.random.normal(0, stddev, n_out)\n",
    "        variance = 2 / (n_in + n_out)\n",
    "        stddev = np.sqrt(variance)\n",
    "        return np.random.normal(0, stddev, (n_in, n_out))\n",
    "        \n",
    "    def he_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            variance = 2 / n_in\n",
    "            stddev = np.sqrt(variance)\n",
    "            return np.random.normal(0, stddev, n_in)\n",
    "        variance = 2 / n_in\n",
    "        stddev = np.sqrt(variance)\n",
    "        return np.random.normal(0, stddev, (n_in, n_out))\n",
    "    \n",
    "    def zeros_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            return np.zeros(n_in)\n",
    "        return np.zeros((n_in, n_out))\n",
    "    \n",
    "    def uniform_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            return np.random.uniform(0, 1, n_in)\n",
    "        return np.random.uniform(0, 1, (n_in, n_out))\n",
    "    \n",
    "    def feedforward(self, a, return_activations=False):\n",
    "        if return_activations:\n",
    "            activations = [a]\n",
    "            for w, b, func in zip(self.weights, self.biases, self.activation_funcs):\n",
    "                z = np.dot(a, w) + b\n",
    "                a = func(z)\n",
    "                activations.append(a)\n",
    "            return activations\n",
    "        else:\n",
    "            for w, b, func in zip(self.weights, self.biases, self.activation_funcs):\n",
    "                z = np.dot(a, w) + b\n",
    "                a = func(z)\n",
    "            return a\n",
    "    \n",
    "    def predict(self, X, label=False):\n",
    "        \"\"\" if to use label_predictions function \"\"\"\n",
    "        if label:\n",
    "            return label_predictions(self.feedforward(X))\n",
    "        else:\n",
    "            return self.feedforward(X)\n",
    "    \n",
    "    def mse(self, X, y, resize=False, denormalize=None):\n",
    "        \"\"\"\n",
    "        first predictions are made, then denormalized and then mse is calculated\n",
    "        \n",
    "        denormalize - a tuple (mean, std)\n",
    "        \"\"\"\n",
    "        if resize:\n",
    "            X = X.to_numpy().reshape(-1, 1)\n",
    "            y = y.to_numpy().reshape(-1, 1)\n",
    "        predictions = self.predict(X)\n",
    "        if denormalize:\n",
    "            predictions = destandardize_data(predictions, denormalize)\n",
    "        return np.mean((predictions - y) ** 2)\n",
    "    \n",
    "    def f1_score(self, X, y_true, average='weighted'):\n",
    "        \"\"\"\n",
    "        calculates f1 score. \n",
    "        y_true : 2 dimensional array with probabilities\n",
    "        average: 'weighted', 'macro'\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        y_pred = label_predictions(predictions)\n",
    "        y_true = label_predictions(y_true)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        return f1\n",
    "\n",
    "    def cross_entropy(self, X, y):\n",
    "        \"\"\" Calculates cross entropy loss \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        epsilon = 1e-9  # Small constant to avoid taking the logarithm of zero\n",
    "        return -np.mean(np.sum(y * np.log(predictions + epsilon), axis=1))\n",
    "    \n",
    "    def loss(self, X, y, resize=False, denormalize=None, test=False):\n",
    "        \"\"\" returns appriopriate loss \"\"\"\n",
    "        if self.model_type == 'regression':\n",
    "            return self.mse(X, y, resize=resize, denormalize=denormalize)\n",
    "        elif self.model_type == 'classification':\n",
    "            if test:\n",
    "                return self.f1_score(X, y)\n",
    "            else:\n",
    "                return self.cross_entropy(X, y)\n",
    "    \n",
    "    def plot_classes(self, data):\n",
    "        \"\"\" plots predicted classes for classifcation data. plot based only on first two columns of passed data. \"\"\"\n",
    "        colors = self.predict(data, label=True)\n",
    "        sns.scatterplot(x=data[:,0], y=data[:,1], hue=colors)\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"backpropagation, returns partial derevatives\"\"\"\n",
    "        # feedforward\n",
    "        activations = self.feedforward(X, return_activations=True)\n",
    "        deltas = [None] * len(self.weights)\n",
    "        # output error\n",
    "        if self.activations[-1] == \"softmax\":\n",
    "            deltas[-1] = activations[-1] - y \n",
    "        elif self.activations[-1] == \"linear\": # linear\n",
    "            deltas[-1] = activations[-1] - y\n",
    "        elif self.activations[-1] == \"sigmoid\":\n",
    "            deltas[-1] = (activations[-1] - y) * self.sigmoid_derivative(activations[-1])\n",
    "        elif self.activations[-1] == \"relu\" or self.activations[-1] == \"tanh\":\n",
    "            raise AssertionError('tanh and relu cant be used on the last layer')\n",
    "        # calculate neurons' errors using backpropagation \n",
    "        for i in reversed(range(len(deltas) - 1)):\n",
    "            if self.activations[i] == \"sigmoid\":\n",
    "                # calculate error delta_l = delta_{l+1} x w_{l+1} * sigmoid'(a_{l})\n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T) * self.sigmoid_derivative(activations[i+1])\n",
    "            elif self.activations[i] == \"linear\":\n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T)\n",
    "            elif self.activations[i] == \"relu\":\n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T) * self.relu_derivative(activations[i+1])\n",
    "            elif self.activations[i] == \"tanh\":    \n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T) * self.tanh_derivative(activations[i+1])\n",
    "            elif self.activations[i] == \"softmax\":\n",
    "                raise AssertionError(\"softmax can be used only on the last layer\")\n",
    "        L = len(self.weights)\n",
    "        weights_gradient = [None] * L\n",
    "        biases_gradient = [None] * L\n",
    "        # calculate partial derevatives of cost function\n",
    "        for i in range(L):\n",
    "            # activations array is longer (so activations[l] in reality is a_{l-1})\n",
    "            weights_gradient[i] = np.dot(activations[i].T, deltas[i])\n",
    "            biases_gradient[i] = np.sum(deltas[i], axis=0)\n",
    "            \n",
    "        return weights_gradient, biases_gradient\n",
    "        \n",
    "    def update_weights(self, X, y, learning_rate, optimize=None, optimize_param=0.9):\n",
    "        \"\"\"\n",
    "        updates weights using gradient descent\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        weights_gradient, biases_gradient = self.backward(X, y)\n",
    "        for i in range(len(self.weights)):\n",
    "            \n",
    "            if optimize is None:\n",
    "                self.weights[i] = self.weights[i] - (learning_rate/m) * weights_gradient[i]\n",
    "                self.biases[i] = self.biases[i] - (learning_rate/m) * biases_gradient[i]\n",
    "            \n",
    "            elif optimize == 'moment':\n",
    "                self.velocities_weights[i] = weights_gradient[i] + self.velocities_weights[i] * optimize_param\n",
    "                self.velocities_biases[i] = biases_gradient[i] + self.velocities_biases[i] * optimize_param\n",
    "            \n",
    "                self.weights[i] = self.weights[i] - (learning_rate/m) * self.velocities_weights[i]\n",
    "                self.biases[i] = self.biases[i] - (learning_rate/m) * self.velocities_biases[i]            \n",
    "            \n",
    "            elif optimize == 'rmsprop':\n",
    "                self.velocities_weights[i] = optimize_param * self.velocities_weights[i] + (1-optimize_param) * (weights_gradient[i]**2)\n",
    "                self.velocities_biases[i] = optimize_param * self.velocities_biases[i] + (1-optimize_param) * (biases_gradient[i]**2)\n",
    "                \n",
    "                self.weights[i] = self.weights[i] - (learning_rate/m) * (weights_gradient[i]/np.sqrt(self.velocities_weights[i]))\n",
    "                self.biases[i] = self.biases[i] - (learning_rate/m) * (biases_gradient[i]/np.sqrt(self.velocities_biases[i]))\n",
    "    \n",
    "    def create_batches(self, X, y, batch_size):\n",
    "        \"\"\" creates batches \"\"\"\n",
    "        N = len(X)\n",
    "        combined_data = list(zip(X, y))\n",
    "        np.random.shuffle(combined_data)\n",
    "\n",
    "        X_shuffled = np.array(list(zip(*combined_data))[0])\n",
    "        y_shuffled = np.array(list(zip(*combined_data))[1])\n",
    "\n",
    "        X_mini_batches = [X_shuffled[k:k+batch_size] for k in range(0, N, batch_size)]\n",
    "        y_mini_batches = [y_shuffled[k:k+batch_size] for k in range(0, N, batch_size)]\n",
    "        return X_mini_batches, y_mini_batches\n",
    "        \n",
    "    def train(self, X, y, learning_rate=0.001, epochs=10000, batch_size=None, return_history=False, \n",
    "              optimize=None, optimize_param=0.9, \n",
    "              X_test=None, y_test=None, cutoff=None, cutoff_inverse=False, resize=False, denormalize=None,\n",
    "              info_interval=500, mute_print=False):\n",
    "        \"\"\"\n",
    "        To test and print MSE on testset, X_test and y_test need to be provided\n",
    "        To set early stopping, provide cutoff (and test sets)\n",
    "        denormalize: a tuple (mean, std) for denormalizing target for calculating mse\n",
    "        info_interval: number of epochs between logging, set to None to turn off console output\n",
    "        optimize: 'moment' or 'rmsprop'\n",
    "        \"\"\"\n",
    "        assert optimize is None or optimize in ['moment', 'rmsprop'], f\"optimize should be None or in ['moment', 'rmsprop']\"\n",
    "        \n",
    "        # setting flags\n",
    "        test_mse = False\n",
    "        if X_test is not None and y_test is not None:\n",
    "            test_mse = True\n",
    "        early_stopping = False\n",
    "        if test_mse and cutoff is not None:\n",
    "            early_stopping = True\n",
    "        \n",
    "        # resizing\n",
    "        if resize:\n",
    "            X = X.to_numpy().reshape(-1, 1)\n",
    "            y = y.to_numpy().reshape(-1, 1)\n",
    "            if test_mse:\n",
    "                X_test = X_test.to_numpy().reshape(-1, 1)\n",
    "                y_test = y_test.to_numpy().reshape(-1, 1)\n",
    "            \n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "        \n",
    "        # denormalizing for testing\n",
    "        y_mse = y\n",
    "        if denormalize is not None:\n",
    "            y_mse = destandardize_data(y, denormalize)\n",
    "            \n",
    "        history = History()\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            # mini batch\n",
    "            if batch_size < len(X):\n",
    "                batches_x, batches_y = self.create_batches(X, y, batch_size)\n",
    "                for i in range(len(batches_x)):\n",
    "                    self.update_weights(batches_x[i], batches_y[i], learning_rate, optimize, optimize_param)\n",
    "            else:\n",
    "                self.update_weights(X, y, learning_rate, optimize, optimize_param)\n",
    "            \n",
    "            # logging\n",
    "            if info_interval is not None and epoch % info_interval == 0:\n",
    "                loss = self.loss(X, y_mse, denormalize=denormalize)\n",
    "                history.loss.append(loss)\n",
    "                history.loss_epochs.append(epoch)\n",
    "                training_info = f\"Epoch {epoch}: Loss = {round(loss, 3)}\"\n",
    "                if test_mse:\n",
    "                    loss_test = self.loss(X_test, y_test, denormalize=denormalize, test=True)   \n",
    "                    history.test_loss.append(loss_test)\n",
    "                    training_info += f\" Test Loss = {round(loss_test, 3)}\"\n",
    "                if not mute_print:\n",
    "                    print(training_info)\n",
    "            \n",
    "            history.weights.append(copy.deepcopy(self.weights))\n",
    "            history.biases.append(copy.deepcopy(self.biases))\n",
    "            \n",
    "            # early stopping\n",
    "            if early_stopping:\n",
    "                loss_es = self.loss(X_test, y_test, denormalize=denormalize, test=True)\n",
    "                if (loss_es <= cutoff and not cutoff_inverse) or (loss_es >= cutoff and cutoff_inverse):\n",
    "                    loss = self.loss(X, y_mse, denormalize=denormalize)    \n",
    "                    training_info = f\"Epoch {epoch}: Loss = {round(loss, 3)}\"\n",
    "                    training_info += f\" Test Loss = {round(loss_es, 3)}\"\n",
    "                    print(training_info)\n",
    "                    break\n",
    "        \n",
    "        if return_history:\n",
    "            history.model_type = self.model_type\n",
    "            return history\n",
    "\n",
    "def standardize_data(X):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    X_new - standardized X\n",
    "    a tuple (mean, std) - normal distribution parameters from X for destandarizing\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_new = (X - mean) / std\n",
    "    return X_new, (mean, std)\n",
    "\n",
    "def destandardize_data(X, parameters):\n",
    "    \"\"\"parameters: a tuple (mean, std)\"\"\"\n",
    "    return X * parameters[1] + parameters[0]\n",
    "\n",
    "\n",
    "def label_predictions(y_pred, class_order=None):\n",
    "    \"\"\"\n",
    "    y_pred : 2 dimensional array\n",
    "    class_order : default [..., 2, 1, 0]\n",
    "    \"\"\"\n",
    "    if class_order is None:\n",
    "        class_order = np.array(range(len(y_pred[0])))\n",
    "    max_indices = np.argmax(y_pred, axis=1)\n",
    "    return class_order[max_indices]\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    \"\"\" y : pd.Series \"\"\"\n",
    "    return pd.get_dummies(y, dtype=int).to_numpy()\n",
    "\n",
    "def plot_classes(data, colors):\n",
    "    sns.scatterplot(x=data[:,0], y=data[:,1], hue=colors)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\" calculated mse \"\"\"\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\" calculates cross entropy loss, y_true and y_pred should be 2-dimensional \"\"\"\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "class History:\n",
    "    \"\"\"\n",
    "    Class which attributes are weights from all training epochs \n",
    "    and loss for chosen loss_epochs (usually epochs with some interval)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss = []\n",
    "        self.test_loss = []\n",
    "        self.loss_epochs = []\n",
    "        self.model_type = 'regression'\n",
    "    \n",
    "    def plot_weights(self, layer, bias=False, return_plot=False):\n",
    "        \"\"\"\n",
    "        layer - integer, index of weights vector\n",
    "        if to plot bias instead of weights \n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        weights_type = 'Weights'\n",
    "        if bias:\n",
    "            weights_type = 'Biases'\n",
    "            for epoch in self.biases:\n",
    "                weights.append(epoch[layer].flatten())\n",
    "        else:\n",
    "            for epoch in self.weights:\n",
    "                weights.append(epoch[layer].flatten())\n",
    "\n",
    "        weights = np.array(weights)\n",
    "        for i in range(weights.shape[1]):\n",
    "            w_y = weights[:,i]\n",
    "            plt.plot(range(len(self.weights)), w_y)\n",
    "            \n",
    "        plt.xlabel('Epochs')\n",
    "        plt.title(f'{weights_type} values vs epochs at layer {layer}')\n",
    "        if return_plot:\n",
    "            return plt\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_loss(self, test=False, return_plot=False, settings=''):\n",
    "        if test:\n",
    "            plt.plot(self.loss_epochs, self.test_loss, settings)\n",
    "            plt.title('Test loss history')\n",
    "        else:\n",
    "            plt.plot(self.loss_epochs, self.loss, settings)\n",
    "            plt.title('Training loss history')\n",
    "        plt.xlabel('Epochs')\n",
    "        if self.model_type == 'classification':\n",
    "            plt.ylabel('Cross Entropy')\n",
    "        else: \n",
    "            plt.ylabel('MSE')\n",
    "        if return_plot:\n",
    "            return plt\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27fd33",
   "metadata": {},
   "source": [
    "## Eksperymenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb42dc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmultimodal-large\\nsteps-large\\nrings5-regular\\nrings3-regular\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "multimodal-large\n",
    "steps-large\n",
    "rings5-regular\n",
    "rings3-regular\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

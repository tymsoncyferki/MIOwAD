{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b56a3f",
   "metadata": {},
   "source": [
    "# NN2\n",
    "Tymoteusz Urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "530cdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "a8e79e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, weights=None, biases=None, activations=None):\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        \n",
    "        if weights is None:\n",
    "            # self.weights = [self.xavier_init(layers[i-1], layers[i]) for i in range(1, self.num_layers)]\n",
    "            self.weights = [np.random.randn(layers[i-1], layers[i]) for i in range(1, self.num_layers)]\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if biases is None:\n",
    "            # self.biases = [self.xavier_bias_init(layers[i]) for i in range(1, self.num_layers)]\n",
    "            self.biases = [np.random.randn(layers[i]) for i in range(1, self.num_layers)]\n",
    "        else:\n",
    "            self.biases = biases\n",
    "        \n",
    "        if activations is None:\n",
    "            self.activations = ['sigmoid' for i in range(1, self.num_layers - 1)] + ['linear']\n",
    "        else:\n",
    "            self.activations = activations\n",
    "        \n",
    "        activation_functions = {\n",
    "            'sigmoid': self._sigmoid,\n",
    "            'linear': self._linear\n",
    "        }\n",
    "        self.activation_funcs = list(map(lambda x: activation_functions.get(x), self.activations))\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _linear(self, z):\n",
    "        return z\n",
    "    \n",
    "    def xavier_init(self, n_in, n_out):\n",
    "        variance = 2.0 / (n_in + n_out)\n",
    "        stddev = np.sqrt(variance)\n",
    "        return np.random.normal(0, stddev, (n_in, n_out))\n",
    "    \n",
    "    def xavier_bias_init(self, n_out):\n",
    "        variance = 1.0 / n_out\n",
    "        stddev = np.sqrt(variance)\n",
    "        return np.random.normal(0, stddev, n_out)\n",
    "    \n",
    "    def feedforward(self, a, return_activations=False):\n",
    "        if return_activations:\n",
    "            activations = [a]\n",
    "            for w, b, func in zip(self.weights, self.biases, self.activation_funcs):\n",
    "                z = np.dot(a, w) + b\n",
    "                a = func(z)\n",
    "                activations.append(a)\n",
    "            return activations\n",
    "        else:\n",
    "            for w, b, func in zip(self.weights, self.biases, self.activation_funcs):\n",
    "                z = np.dot(a, w) + b\n",
    "                a = func(z)\n",
    "            return a\n",
    "            \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.feedforward(X)\n",
    "    \n",
    "    def mse(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean((predictions - y) ** 2)\n",
    "    \n",
    "    def sigmoid_derivative(self, a):\n",
    "        \"\"\"calculates sigm'(z) where a = sigm(z)\"\"\"\n",
    "        return a * (1-a)\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        \"\"\"backpropagation, returns partial derevatives\"\"\"\n",
    "        # feedforward\n",
    "        activations = self.feedforward(X, return_activations=True)\n",
    "        deltas = [None] * len(self.weights)\n",
    "        # output error\n",
    "        deltas[-1] = activations[-1] - y.reshape(-1, 1)\n",
    "        # calculate neurons' errors using backpropagation \n",
    "        for i in reversed(range(len(deltas) - 1)):\n",
    "            if self.activations[i] == \"sigmoid\":\n",
    "                # calculate error delta_l = delta_{l+1} x w_{l+1} * sigmoid'()\n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T) * self.sigmoid_derivative(activations[i+1])\n",
    "            elif self.activations[i] == \"linear\":\n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T)\n",
    "        \n",
    "        L = len(self.weights)\n",
    "        weights_gradient = [None] * L\n",
    "        biases_gradient = [None] * L\n",
    "        # calculate partial derevatives of cost function\n",
    "        for i in range(L):\n",
    "            weights_gradient[i] = np.dot(activations[i].T, deltas[i])\n",
    "            biases_gradient[i] = np.mean(deltas[i], axis=0)\n",
    "            \n",
    "        return weights_gradient, biases_gradient\n",
    "        \n",
    "    def update_weights(self, X, y, learning_rate):\n",
    "        \"\"\"\n",
    "        updates weights using gradient descent\n",
    "        \"\"\"\n",
    "        weights_gradient, biases_gradient = self.backward(X, y, learning_rate)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * weights_gradient[i]\n",
    "            self.biases[i] -= learning_rate * biases_gradient[i]\n",
    "    \n",
    "    def create_batches(self, X, y, batch_size):\n",
    "        N = len(X)\n",
    "        combined_data = np.array(list(zip(X, y)))\n",
    "        np.random.shuffle(combined_data)\n",
    "\n",
    "        X_shuffled = np.array(list(zip(*combined_data))[0])\n",
    "        y_shuffled = np.array(list(zip(*combined_data))[1])\n",
    "\n",
    "        X_mini_batches = [X_shuffled[k:k+batch_size] for k in range(0, N, batch_size)]\n",
    "        y_mini_batches = [y_shuffled[k:k+batch_size] for k in range(0, N, batch_size)]\n",
    "        return X_mini_batches, y_mini_batches\n",
    "        \n",
    "    def train(self, X, y, learning_rate, epochs, batch_size=len(X)):        \n",
    "        for epoch in range(epochs):\n",
    "            if batch_size < len(X):\n",
    "                batches_x, batches_y = self.create_batches(X, y, batch_size)\n",
    "                for i in range(len(batches_x)):\n",
    "                    self.update_weights(batches_x[i], batches_y[i], learning_rate)\n",
    "            else:\n",
    "                self.update_weights(X, y, learning_rate)\n",
    "            if epoch % 500 == 0:\n",
    "                loss = self.mse(X, y)\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "59652dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_simple = pd.read_csv(\"../dane/regression/square-simple-test.csv\", index_col=0)\n",
    "square_simple_train = pd.read_csv(\"../dane/regression/square-simple-training.csv\", index_col=0)\n",
    "steps_large = pd.read_csv(\"../dane/regression/steps-large-test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "068c20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = NeuralNetwork([1,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "8bbc3819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 8729.975635007862\n",
      "Epoch 500: Loss = 5685.0014672768\n",
      "Epoch 1000: Loss = 4985.930265035239\n",
      "Epoch 1500: Loss = 4561.623910438382\n",
      "Epoch 2000: Loss = 4186.410473091892\n",
      "Epoch 2500: Loss = 3825.4982756698946\n",
      "Epoch 3000: Loss = 3422.737396355571\n",
      "Epoch 3500: Loss = 3040.423235003524\n",
      "Epoch 4000: Loss = 2679.7756572515273\n",
      "Epoch 4500: Loss = 2345.821707913178\n",
      "Epoch 5000: Loss = 2046.6042677897997\n",
      "Epoch 5500: Loss = 1785.1975159683102\n",
      "Epoch 6000: Loss = 1560.537878321971\n",
      "Epoch 6500: Loss = 1369.3547177770115\n",
      "Epoch 7000: Loss = 1207.4761411097063\n",
      "Epoch 7500: Loss = 1070.6083794549556\n",
      "Epoch 8000: Loss = 954.7437858849828\n",
      "Epoch 8500: Loss = 856.346536927339\n",
      "Epoch 9000: Loss = 772.4170752764475\n",
      "Epoch 9500: Loss = 700.4552983711344\n",
      "Epoch 10000: Loss = 638.4052179876758\n",
      "Epoch 10500: Loss = 584.5951708147759\n",
      "Epoch 11000: Loss = 537.6472365063645\n",
      "Epoch 11500: Loss = 496.4324693170104\n",
      "Epoch 12000: Loss = 460.018505260388\n",
      "Epoch 12500: Loss = 427.6328104751584\n",
      "Epoch 13000: Loss = 398.63756079133253\n",
      "Epoch 13500: Loss = 372.5149970745304\n",
      "Epoch 14000: Loss = 348.83328668966783\n",
      "Epoch 14500: Loss = 327.2759480209263\n",
      "Epoch 15000: Loss = 307.61735877458085\n",
      "Epoch 15500: Loss = 289.722598167244\n",
      "Epoch 16000: Loss = 273.45959951973555\n",
      "Epoch 16500: Loss = 258.67960438149885\n",
      "Epoch 17000: Loss = 245.20743627515054\n",
      "Epoch 17500: Loss = 232.89040385396427\n",
      "Epoch 18000: Loss = 221.5836594338067\n",
      "Epoch 18500: Loss = 211.15898189153958\n",
      "Epoch 19000: Loss = 201.50901403464658\n",
      "Epoch 19500: Loss = 192.54705066700882\n"
     ]
    }
   ],
   "source": [
    "mlp.train(square_simple['x'].to_numpy().reshape(-1, 1), square_simple['y'].to_numpy().reshape(-1, 1), 0.00001, 20000, 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

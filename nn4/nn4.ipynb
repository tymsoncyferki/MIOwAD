{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac49a10d",
   "metadata": {},
   "source": [
    "# NN4\n",
    "Tymoteusz Urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530cdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e79e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    \"\"\"\n",
    "    Implementation of multi layer perceptron\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    layers : List\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" dodac opcje classification \"\"\"\n",
    "    def __init__(self, layers, weights=None, biases=None, activations=None, initalization='xavier', model_type='regression'):\n",
    "        \"\"\"\n",
    "        activations - available functions: 'sigmoid', 'linear'\n",
    "        initialization - available types: 'xavier', 'he', 'uniform'\n",
    "        model_type - 'regression' and 'classification'\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        \n",
    "        initialization_functions = {\n",
    "            'xavier': self.xavier_init,\n",
    "            'he': self.he_init,\n",
    "            'uniform': self.uniform_init\n",
    "        }\n",
    "        self.init_function = initialization_functions.get(initalization)\n",
    "        \n",
    "        if weights is None:\n",
    "            self.weights = [self.init_function(layers[i-1], layers[i]) for i in range(1, self.num_layers)]\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        if biases is None:\n",
    "            self.biases = [self.init_function(layers[i]) for i in range(1, self.num_layers)]\n",
    "        else:\n",
    "            self.biases = biases\n",
    "        \n",
    "        if activations is None:\n",
    "            if self.model_type == 'regression':\n",
    "                self.activations = ['sigmoid' for i in range(1, self.num_layers - 1)] + ['linear']\n",
    "            elif self.model_type == 'classification':\n",
    "                self.activations = ['sigmoid' for i in range(1, self.num_layers - 1)] + ['softmax']\n",
    "        else:\n",
    "            self.activations = activations\n",
    "        \n",
    "        self.velocities_weights = [self.zeros_init(layers[i-1], layers[i]) for i in range(1, self.num_layers)]\n",
    "        self.velocities_biases = [self.zeros_init(layers[i]) for i in range(1, self.num_layers)]\n",
    "        \n",
    "        activation_functions = {\n",
    "            'sigmoid': self._sigmoid,\n",
    "            'linear': self._linear\n",
    "        }\n",
    "        self.activation_funcs = list(map(lambda x: activation_functions.get(x), self.activations))\n",
    "        \n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _linear(self, z):\n",
    "        return z\n",
    "    \n",
    "    \"\"\"def _softmax\"\"\"\n",
    "    \n",
    "    def xavier_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            n_out = n_in\n",
    "            variance = 1 / n_out\n",
    "            stddev = np.sqrt(variance)\n",
    "            return np.random.normal(0, stddev, n_out)\n",
    "        variance = 2 / (n_in + n_out)\n",
    "        stddev = np.sqrt(variance)\n",
    "        return np.random.normal(0, stddev, (n_in, n_out))\n",
    "        \n",
    "    def he_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            variance = 2 / n_in\n",
    "            stddev = np.sqrt(variance)\n",
    "            return np.random.normal(0, stddev, n_in)\n",
    "        variance = 2 / n_in\n",
    "        stddev = np.sqrt(variance)\n",
    "        return np.random.normal(0, stddev, (n_in, n_out))\n",
    "    \n",
    "    def zeros_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            return np.zeros(n_in)\n",
    "        return np.zeros((n_in, n_out))\n",
    "    \n",
    "    def uniform_init(self, n_in, n_out=None):\n",
    "        if n_out is None:\n",
    "            return np.random.uniform(0, 1, n_in)\n",
    "        return np.random.uniform(0, 1, (n_in, n_out))\n",
    "    \n",
    "    def feedforward(self, a, return_activations=False):\n",
    "        if return_activations:\n",
    "            activations = [a]\n",
    "            for w, b, func in zip(self.weights, self.biases, self.activation_funcs):\n",
    "                z = np.dot(a, w) + b\n",
    "                a = func(z)\n",
    "                activations.append(a)\n",
    "            return activations\n",
    "        else:\n",
    "            for w, b, func in zip(self.weights, self.biases, self.activation_funcs):\n",
    "                z = np.dot(a, w) + b\n",
    "                a = func(z)\n",
    "            return a\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.feedforward(X)\n",
    "    \n",
    "    def mse(self, X, y, resize=False, denormalize=None):\n",
    "        \"\"\"\n",
    "        first predictions are made, then denormalized and then mse is calculated\n",
    "        \n",
    "        denormalize - a tuple (mean, std)\n",
    "        \"\"\"\n",
    "        if resize:\n",
    "            X = X.to_numpy().reshape(-1, 1)\n",
    "            y = y.to_numpy().reshape(-1, 1)\n",
    "        predictions = self.predict(X)\n",
    "        if denormalize:\n",
    "            predictions = destandardize_data(predictions, denormalize)\n",
    "        return np.mean((predictions - y) ** 2)\n",
    "    \n",
    "    def sigmoid_derivative(self, a):\n",
    "        \"\"\"calculates sigm'(z) where a = sigm(z)\"\"\"\n",
    "        return a * (1-a)\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"backpropagation, returns partial derevatives\"\"\"\n",
    "        # feedforward\n",
    "        activations = self.feedforward(X, return_activations=True)\n",
    "        deltas = [None] * len(self.weights)\n",
    "        # output error\n",
    "        deltas[-1] = activations[-1] - y.reshape(-1, 1)  \n",
    "        \"\"\" tutaj output error tez powinien zalezec od warstwy wyjsciowej \"\"\"\n",
    "        # calculate neurons' errors using backpropagation \n",
    "        for i in reversed(range(len(deltas) - 1)):\n",
    "            if self.activations[i] == \"sigmoid\":\n",
    "                # calculate error delta_l = delta_{l+1} x w_{l+1} * sigmoid'(a_{l+1})\n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T) * self.sigmoid_derivative(activations[i+1])\n",
    "            elif self.activations[i] == \"linear\":\n",
    "                deltas[i] = np.dot(deltas[i+1], self.weights[i+1].T)\n",
    "        \n",
    "        L = len(self.weights)\n",
    "        weights_gradient = [None] * L\n",
    "        biases_gradient = [None] * L\n",
    "        # calculate partial derevatives of cost function\n",
    "        for i in range(L):\n",
    "            # activations array is longer (so activations[l] in reality is a_{l-1})\n",
    "            weights_gradient[i] = np.dot(activations[i].T, deltas[i])\n",
    "            biases_gradient[i] = np.sum(deltas[i], axis=0)\n",
    "            \n",
    "        return weights_gradient, biases_gradient\n",
    "        \n",
    "    def update_weights(self, X, y, learning_rate, optimize=None, optimize_param=0.9):\n",
    "        \"\"\"\n",
    "        updates weights using gradient descent\n",
    "        \"\"\"\n",
    "        m = len(X)\n",
    "        weights_gradient, biases_gradient = self.backward(X, y)\n",
    "        for i in range(len(self.weights)):\n",
    "            \n",
    "            if optimize is None:\n",
    "                self.weights[i] = self.weights[i] - (learning_rate/m) * weights_gradient[i]\n",
    "                self.biases[i] = self.biases[i] - (learning_rate/m) * biases_gradient[i]\n",
    "            \n",
    "            elif optimize == 'moment':\n",
    "                self.velocities_weights[i] = weights_gradient[i] + self.velocities_weights[i] * optimize_param\n",
    "                self.velocities_biases[i] = biases_gradient[i] + self.velocities_biases[i] * optimize_param\n",
    "            \n",
    "                self.weights[i] = self.weights[i] - (learning_rate/m) * self.velocities_weights[i]\n",
    "                self.biases[i] = self.biases[i] - (learning_rate/m) * self.velocities_biases[i]            \n",
    "            \n",
    "            elif optimize == 'rmsprop':\n",
    "                self.velocities_weights[i] = optimize_param * self.velocities_weights[i] + (1-optimize_param) * (weights_gradient[i]**2)\n",
    "                self.velocities_biases[i] = optimize_param * self.velocities_biases[i] + (1-optimize_param) * (biases_gradient[i]**2)\n",
    "                \n",
    "                self.weights[i] = self.weights[i] - (learning_rate/m) * (weights_gradient[i]/np.sqrt(self.velocities_weights[i]))\n",
    "                self.biases[i] = self.biases[i] - (learning_rate/m) * (biases_gradient[i]/np.sqrt(self.velocities_biases[i]))\n",
    "    \n",
    "    def create_batches(self, X, y, batch_size):\n",
    "        N = len(X)\n",
    "        combined_data = np.array(list(zip(X, y)))\n",
    "        np.random.shuffle(combined_data)\n",
    "\n",
    "        X_shuffled = np.array(list(zip(*combined_data))[0])\n",
    "        y_shuffled = np.array(list(zip(*combined_data))[1])\n",
    "\n",
    "        X_mini_batches = [X_shuffled[k:k+batch_size] for k in range(0, N, batch_size)]\n",
    "        y_mini_batches = [y_shuffled[k:k+batch_size] for k in range(0, N, batch_size)]\n",
    "        return X_mini_batches, y_mini_batches\n",
    "        \n",
    "    def train(self, X, y, learning_rate=0.001, epochs=10000, batch_size=None, return_history=False, \n",
    "              optimize=None, optimize_param=0.9,\n",
    "              X_test=None, y_test=None, cutoff=None, resize=False, denormalize=None, info_interval=500, mute_print=False):\n",
    "        \"\"\"\n",
    "        To test and print MSE on testset, X_test and y_test need to be provided\n",
    "        To set early stopping, provide cutoff (and test sets)\n",
    "        denormalize: a tuple (mean, std) for denormalizing target for calculating mse\n",
    "        info_interval: number of epochs between logging, set to None to turn off console output\n",
    "        optimize: 'moment' or 'rmsprop'\n",
    "        \"\"\"\n",
    "        assert optimize is None or optimize in ['moment', 'rmsprop'], f\"optimize should be None or in ['moment', 'rmsprop']\"\n",
    "        \n",
    "        test_mse = False\n",
    "        if X_test is not None and y_test is not None:\n",
    "            test_mse = True\n",
    "        early_stopping = False\n",
    "        if test_mse and cutoff is not None:\n",
    "            early_stopping = True\n",
    "        \n",
    "        if resize:\n",
    "            X = X.to_numpy().reshape(-1, 1)\n",
    "            y = y.to_numpy().reshape(-1, 1)\n",
    "            if test_mse:\n",
    "                X_test = X_test.to_numpy().reshape(-1, 1)\n",
    "                y_test = y_test.to_numpy().reshape(-1, 1)\n",
    "            \n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "        \n",
    "        y_mse = y\n",
    "        if denormalize is not None:\n",
    "            y_mse = destandardize_data(y, denormalize)\n",
    "            \n",
    "        history = History()\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            if batch_size < len(X):\n",
    "                batches_x, batches_y = self.create_batches(X, y, batch_size)\n",
    "                for i in range(len(batches_x)):\n",
    "                    self.update_weights(batches_x[i], batches_y[i], learning_rate, optimize, optimize_param)\n",
    "            else:\n",
    "                self.update_weights(X, y, learning_rate, optimize, optimize_param)\n",
    "            \n",
    "            if info_interval is not None and epoch % info_interval == 0:\n",
    "                loss = self.mse(X, y_mse, denormalize=denormalize)    \n",
    "                history.loss.append(loss)\n",
    "                history.loss_epochs.append(epoch)\n",
    "                training_info = f\"Epoch {epoch}: Loss = {round(loss, 3)}\"\n",
    "                if test_mse:\n",
    "                    loss_test = self.mse(X_test, y_test, denormalize=denormalize)\n",
    "                    history.test_loss.append(loss_test)\n",
    "                    training_info += f\" Test Loss = {round(loss_test, 3)}\"\n",
    "                if not mute_print:\n",
    "                    print(training_info)\n",
    "            \n",
    "            history.weights.append(copy.deepcopy(self.weights))\n",
    "            history.biases.append(copy.deepcopy(self.biases))\n",
    "            \n",
    "            if early_stopping:\n",
    "                loss_es = self.mse(X_test, y_test, denormalize=denormalize)\n",
    "                if loss_es < cutoff:\n",
    "                    loss = self.mse(X, y_mse, denormalize=denormalize)    \n",
    "                    training_info = f\"Epoch {epoch}: Loss = {round(loss, 3)}\"\n",
    "                    loss_test = self.mse(X_test, y_test, denormalize=denormalize)\n",
    "                    training_info += f\" Test Loss = {round(loss_test, 3)}\"\n",
    "                    print(training_info)\n",
    "                    break\n",
    "        \n",
    "        if return_history:\n",
    "            return history\n",
    "\n",
    "def standardize_data(X):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    X_new - standardized X\n",
    "    a tuple (mean, std) - normal distribution parameters from X for destandarizing\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_new = (X - mean) / std\n",
    "    return X_new, (mean, std)\n",
    "\n",
    "def destandardize_data(X, parameters):\n",
    "    \"\"\"parameters: a tuple (mean, std)\"\"\"\n",
    "    return X * parameters[1] + parameters[0]\n",
    "\n",
    "    \n",
    "class History:\n",
    "    \"\"\"\n",
    "    Class which attributes are weights from all training epochs \n",
    "    and loss for chosen loss_epochs (usually epochs with some interval)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss = []\n",
    "        self.test_loss = []\n",
    "        self.loss_epochs = []\n",
    "    \n",
    "    def plot_weights(self, layer, bias=False, return_plot=False):\n",
    "        \"\"\"\n",
    "        layer - integer, index of weights vector\n",
    "        if to plot bias instead of weights \n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        weights_type = 'Weights'\n",
    "        if bias:\n",
    "            weights_type = 'Biases'\n",
    "            for epoch in self.biases:\n",
    "                weights.append(epoch[layer].flatten())\n",
    "        else:\n",
    "            for epoch in self.weights:\n",
    "                weights.append(epoch[layer].flatten())\n",
    "\n",
    "        weights = np.array(weights)\n",
    "        for i in range(weights.shape[1]):\n",
    "            w_y = weights[:,i]\n",
    "            plt.plot(range(len(self.weights)), w_y)\n",
    "            \n",
    "        plt.xlabel('Epochs')\n",
    "        plt.title(f'{weights_type} values vs epochs at layer {layer}')\n",
    "        if return_plot:\n",
    "            return plt\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "    def plot_loss(self, test=False, return_plot=False):\n",
    "        if test:\n",
    "            plt.plot(self.loss_epochs, self.test_loss)\n",
    "            plt.title('Test loss history')\n",
    "        else:\n",
    "            plt.plot(self.loss_epochs, self.loss)\n",
    "            plt.title('Training loss history')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('MSE')\n",
    "        if return_plot:\n",
    "            return plt\n",
    "        else:\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
